{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk import ne_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello everyone\n"
     ]
    }
   ],
   "source": [
    "chunks=[]\n",
    "sen=input(\"enter your string here \")\n",
    "print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello everyone']\n"
     ]
    }
   ],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(sen)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sen)\n",
    "print(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('hello', 'NN'), ('everyone', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized:\n",
    "    words = nltk.word_tokenize(i)\n",
    "    tagged = nltk.pos_tag(words)\n",
    "    print(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ChunkGram = r\"\"\"\n",
    "            Chunk:\n",
    "            {<NN><NNP>|<NN><NN>|<NN><NNS>|<NNP><NNp>|<NNS><NNP>|<NNS><NNS>?}\n",
    "            {<NN|NNP|NNS>?}\n",
    "            {<IN>}\n",
    "            {<VBZ><DT>}\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkParser = nltk.RegexpParser(ChunkGram)\n",
    "chunked = chunkParser.parse(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S (Chunk hello/NN everyone/NN))\n",
      "['hello', 'hello everyone']\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for subtree in chunked.subtrees(filter=lambda t: t.label() == 'Chunk'):\n",
    "    chunk =\"\"\n",
    "    for leave in subtree.leaves():\n",
    "        chunk += leave[0] + ' '\n",
    "        chunks.append(chunk.strip())\n",
    "print(chunked)\n",
    "print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello']\n"
     ]
    }
   ],
   "source": [
    "finalchunks = []\n",
    "finalnoun = []\n",
    "for idx,item in enumerate(chunks[:-1]):\n",
    "    if (item in chunks[idx+1]):\n",
    "        finalchunks.append(item)\n",
    "print(finalchunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final noun phrase extracted from parser ['hello everyone']\n"
     ]
    }
   ],
   "source": [
    "for i in chunks:\n",
    "       if i not in finalchunks:\n",
    "          finalnoun.append(i)\n",
    "print(\"final noun phrase extracted from parser\", finalnoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
